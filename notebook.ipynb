{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from konlpy.tag import Mecab;tagger=Mecab()\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://arxiv.org/pdf/1703.00955.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.0+751198f'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open('../../dataset/naver_movie_ratings.txt','r',encoding='utf-8').readlines()\n",
    "data = data[1:]\n",
    "data = [[d.split('\\t')[1],d.split('\\t')[2][:-1]] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distibution = [d[1] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(distibution)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEQ_LENGTH=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in data:\n",
    "    t0 = t[0]\n",
    "    t0 = t0.replace(\"<br>\",\"\")\n",
    "    t0 = t0.replace(\"/\",\"\")\n",
    "    \n",
    "    token0 = tagger.morphs(t0)\n",
    "    \n",
    "    if len(token0)>=SEQ_LENGTH:\n",
    "        token0= token0[:SEQ_LENGTH-1]\n",
    "    token0.append(\"<EOS>\")\n",
    "\n",
    "    while len(token0)<SEQ_LENGTH:\n",
    "        token0.append('<PAD>')\n",
    "    \n",
    "    train.append([token0,token0,t[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index={\"<PAD>\":0,\"<SOS>\":1,\"<EOS>\":2,\"<UNK>\":3}\n",
    "\n",
    "for t in train:\n",
    "    for token in t[0]:\n",
    "        if token not in word2index:\n",
    "            word2index[token]=len(word2index)\n",
    "\n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = list(map(lambda w: to_ix[w] if w in to_ix.keys() else to_ix[\"<UNK>\"], seq))\n",
    "    tensor = Variable(torch.LongTensor(idxs)).cuda() if USE_CUDA else Variable(torch.LongTensor(idxs))\n",
    "    return tensor\n",
    "\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x=[]\n",
    "train_y=[]\n",
    "code_labels=[]\n",
    "lengths=[]\n",
    "for tr in train:\n",
    "    temp = prepare_sequence(tr[0], word2index)\n",
    "    temp = temp.view(1,-1)\n",
    "    train_x.append(temp)\n",
    "\n",
    "    temp2 = prepare_sequence(tr[1],word2index)\n",
    "    temp2 = temp2.view(1,-1)\n",
    "    train_y.append(temp2)\n",
    "    \n",
    "    length = [t for t in tr[1] if t !='<PAD>']\n",
    "    lengths.append(len(length))\n",
    "    code_labels.append(Variable(torch.LongTensor([int(tr[2])])).cuda() if USE_CUDA else Variable(torch.LongTensor([int(tr[2])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = list(zip(train_x,train_y,code_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getBatch(batch_size,train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex:eindex]\n",
    "        x,y,c = zip(*batch)\n",
    "        x,y,c = torch.cat(x),torch.cat(y),torch.cat(c)\n",
    "        temp = eindex\n",
    "        eindex = eindex+batch_size\n",
    "        sindex = temp\n",
    "        \n",
    "        yield (x,y,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,latent_size=10,n_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.Wmu= nn.Linear(hidden_size,latent_size)\n",
    "        self.Wsigma = nn.Linear(hidden_size,latent_size)\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,batch_first=True)\n",
    "    \n",
    "    def reparametrize(self, mu, log_var):\n",
    "        \"\"\"\"z = mean + eps * sigma where eps is sampled from N(0, 1).\"\"\"\n",
    "        eps = Variable(torch.randn(mu.size(0), mu.size(1))).cuda() if USE_CUDA else Variable(torch.randn(mu.size(0), mu.size(1)))\n",
    "        z = mu + eps * torch.exp(log_var/2)    # 2 for convert var to std\n",
    "        return z\n",
    "    \n",
    "    def forward(self, input,train=True):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, input.size(0), self.hidden_size)).cuda() if USE_CUDA else Variable(torch.zeros(self.n_layers, input.size(0), self.hidden_size))\n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        mu = self.Wmu(hidden[-1])\n",
    "        log_var = self.Wsigma(hidden[-1])\n",
    "        z = self.reparametrize(mu, log_var)\n",
    "        \n",
    "        return z,mu,log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size,latent_size=10,code_size=2, n_layers=1):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        #self.Wz = nn.Linear(latent_size+code_size,hidden_size)\n",
    "        self.Wz = nn.Linear(latent_size,hidden_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "\n",
    "        #self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, self.n_layers,batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, input,latent,code,lengths,seq_length,training=True):\n",
    "        \n",
    "\n",
    "        embedded = self.embedding(input)\n",
    "        #embedded = self.dropout(embedded)\n",
    "       \n",
    "        # h0\n",
    "        #latent_code = torch.cat((latent,code),1) # z,c\n",
    "        #hidden = self.tanh(self.Wz(latent_code)).view(self.n_layers,input.size(0),-1) \n",
    "        hidden = self.tanh(self.Wz(latent)).view(self.n_layers,input.size(0),-1) \n",
    "        decode=[]\n",
    "        # Apply GRU to the output so far\n",
    "        for i in range(seq_length):\n",
    "            \n",
    "            _, hidden = self.gru(embedded, hidden)\n",
    "            score = self.out(hidden.view(hidden.size(0)*hidden.size(1),-1))\n",
    "            softmaxed = F.log_softmax(score)\n",
    "            decode.append(softmaxed)\n",
    "            _,input = torch.max(softmaxed,1)\n",
    "            embedded = self.embedding(input.unsqueeze(1))\n",
    "            #embedded = self.dropout(embedded)\n",
    "        \n",
    "        # 요고 주의! time-step을 column-wise concat한 후, reshape!!\n",
    "        scores = torch.cat(decode,1)\n",
    "        \n",
    "        return scores.view(input.size(0)*seq_length,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class  Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_num,embed_dim,class_num,kernel_num,kernel_sizes,dropout):\n",
    "        super(Discriminator,self).__init__()\n",
    "        #self.args = args\n",
    "        \n",
    "        V = embed_num # num of vocab\n",
    "        D = embed_dim # dimenstion of word vector\n",
    "        C = class_num # num of class\n",
    "        Ci = 1\n",
    "        Co = kernel_num # 100\n",
    "        Ks = kernel_sizes # [3,4,5]\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        #self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]\n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "        \n",
    "        # kernal_size = (K,D) : D는 단어 벡터 길이라 픽스, K 사이즈만큼 슬라이딩, 스트라이드는 1\n",
    "        \n",
    "        '''\n",
    "        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n",
    "        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n",
    "        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n",
    "        '''\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3) #(N,Co,W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x,train=True):\n",
    "        x = self.embed(x) # (N,W,D)\n",
    "        \n",
    "        #if self.args.static:\n",
    "        #    x = Variable(x)\n",
    "\n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n",
    "\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        '''\n",
    "        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n",
    "        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n",
    "        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n",
    "        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n",
    "        '''\n",
    "        if train:\n",
    "            x = self.dropout(x) # (N,len(Ks)*Co)\n",
    "        logit = self.fc1(x) # (N,C)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 300\n",
    "LATENT_SIZE = 10\n",
    "CODE_SIZE = 2\n",
    "BATCH_SIZE=32\n",
    "STEP=5\n",
    "LEARNING_RATE=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder =  Encoder(len(word2index), HIDDEN_SIZE,LATENT_SIZE, 2)\n",
    "generator = Generator(HIDDEN_SIZE,len(word2index),LATENT_SIZE,CODE_SIZE)\n",
    "discriminator = Discriminator(len(word2index),100,2,30,[3,4,5],0.8)\n",
    "if USE_CUDA:\n",
    "    encoder = encoder.cuda()\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    \n",
    "Recon = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "\n",
    "enc_optim= torch.optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
    "gen_optim = torch.optim.Adam(generator.parameters(),lr=LEARNING_RATE)\n",
    "dis_optiom = torch.optim.Adam(discriminator.parameters(),lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize base VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5] ELBO : 10.8503 , RECON : 10.8503 & KLD : 4.1597\n",
      "[100/5] ELBO : 6.6316 , RECON : 6.6316 & KLD : 10466.3242\n",
      "[200/5] ELBO : 6.4091 , RECON : 6.4091 & KLD : 8144.4893\n",
      "[300/5] ELBO : 6.7089 , RECON : 6.7089 & KLD : 4223.3315\n",
      "[400/5] ELBO : 6.2462 , RECON : 6.2462 & KLD : 3636.0447\n",
      "[500/5] ELBO : 6.3546 , RECON : 6.3546 & KLD : 3336.1274\n",
      "[600/5] ELBO : 6.2122 , RECON : 6.2122 & KLD : 2925.9841\n",
      "[700/5] ELBO : 5.9127 , RECON : 5.9127 & KLD : 2884.1912\n",
      "[800/5] ELBO : 6.1888 , RECON : 6.1888 & KLD : 2921.5837\n",
      "[900/5] ELBO : 5.7738 , RECON : 5.7738 & KLD : 2726.6055\n",
      "[1000/5] ELBO : 5.9214 , RECON : 5.9214 & KLD : 2765.3384\n",
      "[1100/5] ELBO : 5.7488 , RECON : 5.7488 & KLD : 2792.1721\n",
      "[1200/5] ELBO : 5.5326 , RECON : 5.5326 & KLD : 2564.6648\n",
      "[1300/5] ELBO : 5.8342 , RECON : 5.8342 & KLD : 2783.0481\n",
      "[1400/5] ELBO : 5.3264 , RECON : 5.3264 & KLD : 2728.1438\n",
      "[1500/5] ELBO : 5.8936 , RECON : 5.8936 & KLD : 2717.1902\n",
      "[1600/5] ELBO : 5.6877 , RECON : 5.6877 & KLD : 2731.0005\n",
      "[1700/5] ELBO : 5.3395 , RECON : 5.3395 & KLD : 2926.4236\n",
      "[1800/5] ELBO : 5.3914 , RECON : 5.3914 & KLD : 2782.6682\n",
      "[1900/5] ELBO : 5.4671 , RECON : 5.4671 & KLD : 2538.9741\n",
      "[2000/5] ELBO : 5.4231 , RECON : 5.4231 & KLD : 2919.5078\n",
      "[2100/5] ELBO : 5.3322 , RECON : 5.3322 & KLD : 2764.9453\n",
      "[2200/5] ELBO : 5.4090 , RECON : 5.4090 & KLD : 2709.5098\n",
      "[2300/5] ELBO : 5.4221 , RECON : 5.4221 & KLD : 2884.3862\n",
      "[2400/5] ELBO : 5.1808 , RECON : 5.1808 & KLD : 2702.1572\n",
      "[2500/5] ELBO : 5.5511 , RECON : 5.5511 & KLD : 2847.3577\n",
      "[2600/5] ELBO : 5.3061 , RECON : 5.3061 & KLD : 2789.8574\n",
      "[2700/5] ELBO : 5.2017 , RECON : 5.2017 & KLD : 2879.5522\n",
      "[2800/5] ELBO : 5.3103 , RECON : 5.3103 & KLD : 2809.6807\n",
      "[2900/5] ELBO : 5.2402 , RECON : 5.2402 & KLD : 2915.4297\n",
      "[3000/5] ELBO : 5.0527 , RECON : 5.0527 & KLD : 2839.5691\n",
      "[3100/5] ELBO : 5.4738 , RECON : 5.4738 & KLD : 2924.2798\n",
      "[3200/5] ELBO : 5.1352 , RECON : 5.1352 & KLD : 2994.0916\n",
      "[3300/5] ELBO : 5.1663 , RECON : 5.1663 & KLD : 3034.8535\n",
      "[3400/5] ELBO : 5.1933 , RECON : 5.1933 & KLD : 3087.3540\n",
      "[3500/5] ELBO : 5.3496 , RECON : 5.3496 & KLD : 3110.3284\n",
      "[3600/5] ELBO : 5.2714 , RECON : 5.2714 & KLD : 2967.9756\n",
      "[3700/5] ELBO : 5.2034 , RECON : 5.2034 & KLD : 2969.9380\n",
      "[3800/5] ELBO : 5.1064 , RECON : 5.1064 & KLD : 3084.4812\n",
      "[3900/5] ELBO : 5.1695 , RECON : 5.1695 & KLD : 3085.4180\n",
      "[4000/5] ELBO : 5.1463 , RECON : 5.1463 & KLD : 2932.9089\n",
      "[4100/5] ELBO : 5.1441 , RECON : 5.1441 & KLD : 3270.9626\n",
      "[4200/5] ELBO : 5.1788 , RECON : 5.1788 & KLD : 3160.7156\n",
      "[4300/5] ELBO : 5.0035 , RECON : 5.0035 & KLD : 3085.1580\n",
      "[4400/5] ELBO : 4.7676 , RECON : 4.7676 & KLD : 3190.1157\n",
      "[4500/5] ELBO : 5.4973 , RECON : 5.4973 & KLD : 3365.9070\n",
      "[4600/5] ELBO : 5.2984 , RECON : 5.2984 & KLD : 3330.1655\n",
      "[4700/5] ELBO : 4.9783 , RECON : 4.9783 & KLD : 3258.7244\n",
      "[4800/5] ELBO : 5.1070 , RECON : 5.1070 & KLD : 3053.5681\n",
      "[4900/5] ELBO : 5.0858 , RECON : 5.0858 & KLD : 3230.3025\n",
      "[5000/5] ELBO : 5.0099 , RECON : 5.0099 & KLD : 3235.5603\n",
      "[5100/5] ELBO : 5.0871 , RECON : 5.0871 & KLD : 3192.0171\n",
      "[5200/5] ELBO : 5.3334 , RECON : 5.3334 & KLD : 3209.4771\n",
      "[5300/5] ELBO : 5.4609 , RECON : 5.4609 & KLD : 3170.9883\n",
      "[5400/5] ELBO : 5.0019 , RECON : 5.0019 & KLD : 3220.4419\n",
      "[5500/5] ELBO : 4.8375 , RECON : 4.8375 & KLD : 3338.9531\n",
      "[5600/5] ELBO : 4.8584 , RECON : 4.8584 & KLD : 3221.2886\n",
      "[5700/5] ELBO : 4.7587 , RECON : 4.7587 & KLD : 3262.2686\n",
      "[5800/5] ELBO : 5.4709 , RECON : 5.4709 & KLD : 3248.6604\n",
      "[5900/5] ELBO : 5.3004 , RECON : 5.3004 & KLD : 3347.9827\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-b446c3a1a5a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mUSE_CUDA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mgenerator_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device_id, async)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCudaTransfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, device_id, async)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for step in range(STEP):\n",
    "    for i,(x,y,c) in enumerate(getBatch(BATCH_SIZE,train_data)):\n",
    "        #KCA = 0.3\n",
    "        encoder.zero_grad()\n",
    "        generator.zero_grad()\n",
    "\n",
    "        generator_input = Variable(torch.LongTensor([[word2index['<SOS>']]*BATCH_SIZE])).transpose(1,0)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            generator_input = generator_input.cuda()\n",
    "\n",
    "        latent, mu, log_var = encoder(x)\n",
    "        \n",
    "        # 이 때, 코드는 prior p(c)에서 샘플링한다 되있는데, 이게 맞나.. 일단 유니폼 가정\n",
    "        code = Variable(torch.randn([BATCH_SIZE,2]).uniform_(0,1)).cuda() if USE_CUDA else Variable(torch.randn([BATCH_SIZE,2]).uniform_(0,1))\n",
    "\n",
    "        score = generator(generator_input,latent,code,lengths,SEQ_LENGTH)\n",
    "        recon_loss=Recon(score,y.view(-1))\n",
    "        kld_loss = torch.sum(0.5 * (mu**2 + torch.exp(log_var) - log_var -1))\n",
    "\n",
    "    #     KL_COST_ANNEALING\n",
    "        cost_annealing_check = recon_loss.data.cpu().numpy()[0] if USE_CUDA else recon_loss.data.numpy()[0]\n",
    "        if cost_annealing_check<1.5:\n",
    "            KCA = 1.0 # KL cost term annealing\n",
    "\n",
    "        else:\n",
    "            KCA = 0.0\n",
    "        ELBO = recon_loss+KCA*kld_loss\n",
    "\n",
    "        ELBO.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm(encoder.parameters(), 5.0)\n",
    "        torch.nn.utils.clip_grad_norm(generator.parameters(), 5.0)\n",
    "\n",
    "        gen_optim.step()\n",
    "        enc_optim.step()\n",
    "\n",
    "        if i % 100==0:\n",
    "            elbo_for_print = ELBO.data.cpu().numpy()[0] if USE_CUDA else ELBO.data.numpy()[0]\n",
    "            recon_for_print = recon_loss.data.cpu().numpy()[0] if USE_CUDA else recon_loss.data.numpy()[0]\n",
    "            kld_for_print = kld_loss.data.cpu().numpy()[0] if USE_CUDA else kld_loss.data.numpy()[0]\n",
    "            print(\"[%d/%d] ELBO : %.4f , RECON : %.4f & KLD : %.4f\" % (i,STEP,elbo_for_print,\n",
    "                                                                                  recon_for_print,\n",
    "                                                                                  kld_for_print))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 우선 VRAE 초기화가 잘 되는지 체크(kl cost annealing 제대로)\n",
    "* Encoder 진짜 length만\n",
    "* 다른 로스들도 실험\n",
    "* wakeup-sleep 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
